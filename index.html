<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Your Event Website</title>
    <link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>

    <div class="navbar">
        <a href="#introduction">Abstract</a>
        <a href="#schedule">Timeliness</a>
        <a href="#speakers">Applications</a>
        <a href="#organizers">Broad Impact</a>
        <a href="#submissions">Cross Communities</a>
        <a href="#registration">Potential contributors</a>
        <a href="#references">References</a>
    </div>
    </div>

    <div class="container">
        <div class="centered-heading">
            <h1>Special Issue Proposal for the IEEE JSTC</h1>
            <h2>Advances in Deep Neural Network Algorithms and Architectures for Multi-modal Speech Enhancement and
                Separation</h2>
        </div>





        <!-- Your content for each section goes here -->
        <section id="introduction" class="border">
            <h2>Abstract and Relevance</h2>
            <p>Voice is the most commonly used modality by humans to communicate and psychologically blend into society.
                Recent technological advances have triggered the development of various voice-related applications in
                the information and communications technology market. However, noise, reverberation, and interfering
                speech are detrimental for effective communications between humans and other humans or machines, leading
                to performance degradation of associated voice-enabled services. To address the formidable
                speech-in-noise challenge, a range of speech enhancement (SE) and speech separation (SS) techniques are
                normally employed as important front-end speech processing units to handle distortions in input signals
                in order to provide more intelligible speech for automatic speech recognition (ASR), synthesis and
                dialogue systems.
                Emerging advances in artificial intelligence (AI) and machine learning, particularly deep neural
                networks, have led to remarkable improvements in SE and SS based solutions. A growing number of
                researchers have explored various extensions of these methods by utilising a variety of modalities as
                auxiliary inputs to the main speech processing task to access additional information from heterogeneous
                signals. In particular, multi-modal SE and SS systems have been shown to deliver enhanced performance in
                challenging noisy environments by augmenting the conventional speech modality with complementary
                information from multi-sensory inputs, such as video, noise type, signal-to-noise ratio (SNR),
                bone-conducted speech (vibrations), speaker, text information, electromyography, and electromagnetic
                midsagittal articulometer (EMMA) data. Various integration schemes, including early and late fusions,
                cross-attention mechanisms, and self-supervised learning algorithms, have also been successfully
                explored.
                This timely special issue aims to collate latest advances in multi-modal SE and SS systems that exploit
                both conventional and unconventional modalities to further improve state-of-the-art performance in
                benchmark problems. We particularly welcome submissions for novel deep neural network based algorithms
                and architectures, including new feature processing methods for multimodal and cross-modal speech
                processing. We also encourage submissions that address practical issues related to multimodal data
                recording, energy-efficient system design and real-time low-latency solutions, such as for assistive
                hearing and speech communication applications.
                .
            </p>
            <div>
                <p>We anticipate that this special issue can make a valuable contribution to the integration with
                    various subsequent tasks, including</p>
                <ol>
                    <li>Biomedical engineering: assistive hearing technologies </li>
                    <li>Affective computing: multimodal emotion/pathological recognition</li>
                    <li>Human-computer interface: multimodal ASR in noise/reverberant conditions. AR/VR: enhanced
                        speech can provide better quality to AR/VR applications</li>
                </ol>

                <p>We also hold the view that the outcomes of the special issue exhibit a strong connection with
                    various other IEEE societies, such as:</p>
                <ol>
                    <li>IEEE Consumer Technology Society (CTSoc): The subject matter can have relevance to practical
                        applications in consumer electronics. This aspect holds great relevance for IEEE CTSoc.</li>
                    <li>IEEE Engineering in Medicine and Biology Society (EMBS): Applying this to technologies for
                        assistive hearing or speech can contribute to the research in the field of biomedical
                        engineering. This aspect holds great relevance for IEEE EMBS.</li>
                    <li>IEEE Society for Social Implications of Technology (SSIT): The derived assistive hearing or
                        speech devices can provide significant contributions to society. This aspect holds great
                        relevance for IEEE SSIT.</li>
                </ol>


                <p>IEEE Circuits and Systems Society (CSS): The special issue featured novel model architectures
                    designed to implement multimodal speech enhancement in real-world applications, considering
                    computational and time limitations. This aspect holds great relevance for IEEE CSS.
                </p>
            </div>
            <p>Research topics of interest relate to open problems needing addressed. These include, but are not limited
                to, the following.</p>

            <ul>
                <li>Novel acoustic features and architectures for multi-modal SE (MM-SE) and multi-modal SS (MM-SS)
                    solutions. </li>
                <li>The integration of multiple data acquisition devices for multimodal learning and novel learning
                    algorithms robust to imperfect data.</li>
                <li>Few-shot/zero-shot learning and adaptation algorithms for MM-SE and MM-SS systems with a small
                    amount of training and adaptation data. </li>
                <li>Self-supervised and unsupervised learning techniques for MM-SE and MM-SS systems.</li>
                <li>Approaches that effectively reduce model size and inference cost without reducing the speech quality
                    and intelligibility of processed signals.</li>
                <li>Novel objective functions that specifically aim to improve speech intelligibility/quality/automatic
                    speech recognition accuracy. </li>
                <li>Novel applications of MM-SE and MM-SS in human-human and human-machine communications. </li>
                <li>Holistic evaluation metrics for MM-SE and MM-SS systems. </li>
            </ul>

        </section>

        <section id="schedule" class="border">
            <h2>Timeliness</h2>
            <p>In recent years, a growing number of researchers have attempted to incorporate diverse modalities as
                auxiliary inputs for SE and SS models to access additional information. Visual clues represent an
                important modality that carry information complementary to speech signals for everyday communication.
                For example, the McGurk effect refers to cross-effects between visualized mouth or lip shapes and human
                hearing perception. From this perspective, numerous audio-visual MM-SE and MM-SS approaches have been
                proposed.
                Related work by ASR researchers has also demonstrated the potential of audio-visual speech recognition
                to improve the noise robustness of speech recognition in the real world (e.g.
                <a style="color: black;" href="https://doi.org/10.1109/TPAMI.2018.2889052">https://doi.org/10.1109/TPAMI.2018.2889052</a>).
                Such multimodal approaches clearly demonstrate that visual
                cues can successfully enhance the performance of audio-only speech processing. In addition to visual
                information, several studies have proposed contextually incorporating speaker and speaking environment
                modules into SE and SS models. For example, speaking environment information such as SNR and noise types
                has been used to enhance SE models. Other studies have used speaker identity as prior knowledge for
                implementing SE or SS systems.
                A recent timely review of audio-visual based SE and SS approaches was published in the IEEE TASLP, which
                reported a significant and growing number of influential works
                (<a style="color: black;"  href=https://ieeexplore.ieee.org/document/9380418>https://ieeexplore.ieee.org/document/9380418
                </a>). Other notable examples of ongoing globally impactful interdisciplinary research in this area
                include the UK Engineering and Physical Sciences Research Council (EPSRC) funded multi-million pound
                research programme (COG-MHEAR) that is developing real-time cognitively-inspired MM-SE and MM-SS
                approaches to transform the next-generation of assistive hearing and speech communication technology
                (<a style="color: black;" href=http://cogmhear.org>http://cogmhear.org </a>). A related 2023 ICASSP Satellite Workshop was
                recently successfully organised on “Advances in multi-modal hearing assistive technologies (AMHAT)”.
                This complements the world’s first (second) annual Audio-Visual Speech Enhancement Challenge (AVSEC)
                organised as part of the 2023 IEEE ASRU Workshop (<a style="color: black;"
                    href=http://challenge.cogmhear.org>http://challenge.cogmhear.org </a>), and the Speech Enhancement
                for Augmented Reality (SPEAR) Challenge (<a style="color: black;"
                    href=https://imperialcollegelondon.github.io/spear-challenge>https://imperialcollegelondon.github.io/spear-challenge
                </a>). This special issue aims to solicit the latest research in multi-modal SE and SS approaches
                with new benchmark results. We believe it is timely to collate and stimulate new advances in deep
                neural network algorithms and architectures for cross-modal and multi-modal learning for major
                speech signal processing tasks in real-world applications.
            </p>
        </section>
        <section class="border" id="speakers">
            <h2>Applications</h2>
            <p>The MM-SE and MM-SS topics covered in this proposed special issue span a wide range of
                challenging
                real-world applications, including the following.</p>
            <ul>
                <li>Important downstream tasks: automatic speech recognition, speaker and language recognition,
                    voice
                    conversion, and speech synthesis. </li>
                <li>Assistive listening, visual and multimodal speech communication technologies.</li>
                <li>Multimedia processing: multi-modal sentiment analysis, multi-modal dialog systems,
                    multi-modal
                    information retrieval.</li>
                <li>Cognitive robotics, including more natural, multi-modal human-robot interaction and emerging
                    AR/VR
                    applications</li>
                <li>Wearable devices such as smart glasses and chatbots as multimodal communication aids

                </li>
            </ul>
        </section>

        <section class="border" id="organizers">
            <h2>Broad Impact</h2>
            <p>With recent advances in sensing, computing, and communication technologies, vast amounts of audio, text,
                and video data can be accessed easily. Significant efforts have been made to combine complementary
                information from multiple data sources to facilitate improved speech signal processing performance.
                Notable achievements have been demonstrated for multimodal speech processing systems compared to
                mono-modal speech processing. This special issue focuses on advanced artificial intelligence models and
                methods for speech enhancement and separation with heterogeneous data, which we believe will elicit
                broad interest from various research communities, including in data acquisition, text processing,
                computer vision, speech, and multimedia processing. In particular, the unique focus of this special
                issue on cross-modality and multi-modality SE and SS approaches will impact a range of interdisciplinary
                research areas across speech, hearing and language processing, including for robotics, wearable devices,
                more natural human-computer interaction and emerging AR/VR applications.</p>
        </section>

        <section class="border" id="submissions">
            <h2>Cross Communities</h2>
            <p>The proposed special issue spans several areas of the <strong>IEEE SIGNAL PROCESSING SOCIETY.</strong> We
                aim to elicit
                submissions from multidisciplinary fields, specifically data acquisition, text processing, computer
                vision, speech, machine learning, model compression, and multimedia processing.</p>
        </section>

        <section class="border" id="registration">
            <h2>Redundancy</h2>
            <p>We believe our proposal is the first special issue dedicated to advancing research in MM-SE and MM-SS to
                address a range of real-world speech-in-noise challenges. This is evidenced by our review of the
                following related special issues:</p>

            <p> The 2019 IEEE JSTSP Special issue on Far-Field Speech Processing in the Era of Deep Learning” in 2019
                dealt with speech enhancement and separation but did not include multimodal aspects. Similarly the 2020
                IEEE JSTSP Special Issue on “Reconstruction of audio from incomplete or highly degraded observations”
                also dealt with speech enhancement but did not address multimodal aspects. More recently, the 2022 IEEE
                JSTSP Special Issue on “Self-Supervised Learning for Speech and Audio Processing” dealt with general
                speech and audio processing applications but did not address multi-modal speech enhancement and
                separation challenges.</p>

            <p> In the MM context, the 2019 IEEE JSTSP Special Issue on “Deep Learning for Multi-modal Intelligence
                across Speech, Language, Vision, and Heterogeneous Signals” dealt with general MM intelligence across
                speech, language and vision applications, and did not focus on MM speech in noise challenges. More
                recently, the 2022 Special Issue in Language and Cognition (the official journal of the UK Cognitive
                Linguistics Association published by Cambridge University Press) on "Multimodal prosody: speech and
                gesture in interaction" focussed on multimodal prosodic aspects but did not address speech enhancement
                and separation challenges.
            </p>
        </section>

        <section class="border">
            <h2>Potential contributors</h2>
            <p>Researchers expected to contribute original and tutorial/review papers to the SI are listed below:</p>

            <ul>
                <li>Prof Jesper Jensen and Prof Zheng-Hua Tan, Aalborg University, Denmark</li>
                <li>Prof Nima Mesgarani, Columbia University, USA</li>
                <li>Prof Qiang Huang, University of Sunderland, UK</li>
                <li>Prof Bernd T. Meyer, University of Oldenburgh, Germany</li>
                <li>Dr Daniel Michelsanti, Oticon, Denmark</li>
                <li>Prof Dong Yu, Prof Meng Yu and Prof Yong Xu, Tencent AI Lab, USA</li>
                <li>Dr Peter Derleth, Sonova AG</li>
                <li>Prof Sabato Marco Siniscalchi, Norwegian University of Science and Technology</li>
                <li>Prof Jun Du, University of Science and Technology of China (USTC), China</li>
                <li>Prof Ahsan Adeel, University of Wolverhampton, UK</li>
                <li>Dr Erfan Loweimi, University of Cambridge, UK</li>
                <li>Prof Ben Milner, University of East Anglia, UK</li>
                <li>Prof Jun-Cheng Chen, Academia Sinica, Taiwan</li>
                <li>Prof Isabel Trancoso, IST, Univ. Lisbon, Portugal</li>
                <li>Professor Xiao-Lei Zhang, Northwestern Polytechnical University, China</li>
                <li>Professor Chin-Hui Lee, Georgia Institute of Technology, USA</li>
                <li>Prof. Haizhou Li, The Chinese University of Hong Kong, China</li>
                <li>Dr. Gordon Wichern, François G Germain, Sameer Khurana, Chiori Hori, Jonathan LeRoux, Mitsubishi
                    Electric Research Laboratories (MERL)</li>
                <li>Dr. Yoshiki Masuyama, Tokyo Metropolitan University</li>
                <li>Prof. Maja Pantic, Imperial College London, UK</li>
                <li>Prof. Dr Ahsan Adeel, University of Stirling</li>
                <li>Prof. Qingyao Wu, South China University of Technology</li>
                <li>Prof. Xinman Zhang, Xi’an Jiaotong University</li>
                <li>Prof. Hyung-Min Park, Sogang University</li>
                <li>Dr. Ziyi Yang, Microsoft</li>
                <li>Prof. Shigeo Morishima, Waseda University</li>
                <li>Dr. Andrea Lorena Aldana Blanco, University of Edinburgh</li>
            </ul>


            <p>A timely review paper from the guest editors is also planned (provisional title: An Overview of Deep
                Neural Network Based Audio-Visual Speech Enhancement and Separation).</p>
        </section>

        <section class="border">
            <h2>Diversity of Guest Editors</h2>
            <ol>
                <li>
                    <h3>Technical diversity</h3>
                </li>
                <p>Guest editors include experts in multimodal speech signal processing, machine learning, information
                    fusion, multimedia processing and hearing assistive technology.</p>
                <li>
                    <h3>Geographical diversity</h3>
                </li>
                <p>The geographical distribution of the proposed guest editors includes Asia, Europe and North America.
                    The
                    Special Issue co-ordinating guest editors are from the UK and Taiwan.
                </p>
            </ol>
        </section>

        <section class="border">
            <h2>Guest editors and their qualifications</h2>

            <div class="person">
                <div class="person-description">
                    <h2>Amir Hussain</h2>
                    <div class="amir-photo"></div>
                    <p>AMIR HUSSAIN (Lead Guest Editor & Senior Member, IEEE) obtained his BEng (1st Class Honours) and
                        PhD from
                        the University of Strathclyde in Glasgow, UK, in 1992 and 1997 respectively. He is Director of
                        the
                        Centre of AI and Robotics at Edinburgh Napier University, UK. His research interests are
                        cross-disciplinary and industry-led, and include a focus on cognitively-inspired multi-modal
                        speech
                        signal processing for assistive hearing and healthcare technologies. He has co-authored around
                        300
                        journal papers, supervised over 40 PhD students and led major national and international
                        projects. He is
                        currently leading research grants totalling over £5M, including as Chief Investigator of the
                        COG-MHEAR
                        Programme Grant (funded under the UK EPSRC Transformative Healthcare Technologies 2050 Call)
                        that aims
                        to develop truly personalised, multi-modal hearing assistive technology. He is founding Chief
                        Editor of
                        (Springer's) Cognitive Computation journal and is/has been Associate Editor for various other
                        journals
                        including (Elsevier’s) Information Fusion, the IEEE Trans on Neural Networks and Learning
                        Systems, IEEE
                        Trans on Artificial Intelligence, IEEE Trans. on systems, Man Cybernetics: Systems, and IEEE
                        Trans on
                        Emerging Topics in Computational Intelligence. He is founding co-Chair of the UK Special
                        Interest Group
                        on Speech-based Multi-Modal Information Processing (UK-SIGMM), executive committee member of the
                        UK
                        Computing Research Committee (the national expert panel of the IET and BCS for UK computing
                        research).
                        He has served as General Chair of IEEE WCCI 2020 (the world’s largest technical event on
                        computational
                        intelligence, comprising the flagship IJCNN, FUZZ-IEEE and IEEE CEC) and is currently General
                        Chair of
                        the 2023 IEEE Smart World Congress (featuring six co-located IEEE Conferences).</p>
                </div>
            </div>

            <div class="person">
                <div class="person-description">
                    <h2>Yu Tsao</h2>
                    <div class="tsao-photo"></div>
                    <p>YU TSAO (co-Lead Guest Editor & Senior Member, IEEE) received the B.S. and M.S. degrees in
                        electrical
                        engineering from National Taiwan University, Taipei, Taiwan, in 1999 and 2001, respectively, and
                        the
                        Ph.D. degree in electrical and computer engineering from the Georgia Institute of Technology,
                        Atlanta,
                        GA, USA, in 2008. From 2009 to 2011, he was a Researcher with the National Institute of
                        Information and
                        Communications Technology, Tokyo, Japan, where he engaged in research and product development in
                        automatic speech recognition for multilingual speech-to-speech translation. He is currently a
                        Research
                        Fellow (Professor) and the Deputy Director with the Research Center for Information Technology
                        Innovation, Academia Sinica, Taipei. He is also a Jointly Appointed Professor with the
                        Department of
                        Electrical Engineering, Chung Yuan Christian University, Taoyuan, Taiwan. His research interests
                        include
                        assistive oral communication technologies, audio coding, and bio-signal processing. He was the
                        recipient
                        of National Innovation Awards from 2018 to 2021, the Future Tech Breakthrough Award 2019, and
                        the
                        Outstanding Elite Award from the Chung Hwa Rotary Educational Foundation (2019-2020). His papere
                        been
                        awarded the 2021 IEEE Signal Processing Society (SPS), Young Author and Best Paper Awards. He is
                        currently an Associate Editor for the IEEE/ACM Transactions on Audio, Speech and Language
                        Processing and
                        IEEE Signal Processing Letters.</p>
                </div>
            </div>

            <div class="person">
                <div class="person-description">
                    <h2>John H.L. Hansen</h2>
                    <div class="john-photo"></div>
                    <p> JOHN H.L. HANSEN (Fellow, IEEE) received Ph.D. & M.S. degrees from Georgia Institute of
                        Technology, and
                        B.S.E.E. degree from Rutgers Univ. He joined Univ. of Texas at Dallas (UTDallas) in 2005, where
                        he is
                        Associate Dean for Research, Professor of Electrical & Computer Engineering, Distinguished Univ.
                        Chair
                        in Telecommunications Engineering, and holds a joint appointment in School of Behavioral & Brain
                        Sciences (Speech & Hearing). At UTDallas, he established the Center for Robust Speech Systems
                        (CRSS). He
                        is an ISCA Fellow, IEEE Fellow, past Member and TC-Chair of IEEE Signal Proc. Society, Speech &
                        Language
                        Proc. Tech. Comm.(SLTC), and Technical Advisor to U.S. Delegate for NATO (IST/TG-01). He served
                        as ISCA
                        President (2018-2021), and currently serves as Treasurer and ISCA Board Member. He has
                        supervised 99
                        PhD/MS thesis candidates (58 PhD, 41 MS/MA), was recipient of 2020 UT-Dallas Provost’s Award for
                        Graduate Research Mentoring, 2005 Univ. Colorado Teacher Recognition Award, and author/co-author
                        of +865
                        journal/conference papers in the field of speech/language/hearing science, processing &
                        technology with
                        machine learning advancements. He served as General Chair for ISCA INTERSPEECH-2002, Co-Chair
                        for ISCA
                        INTERSPEECH-2022, Co-Organizer and Tech. Chair for IEEE ICASSP-2010, and Co-General Chair and
                        Organizer
                        for IEEE Workshop on Spoken Language Technology (SLT-2014) (Lake Tahoe, NV). He is serving as
                        Tech.
                        Chair for IEEE ICASSP-2024. He received the IEEE Signal Processing Society’s Leo Beranek
                        MERITORIOUS
                        SERVICE AWARD in 2021/22 “for exemplary service to and leadership in the Signal Processing
                        Society.”</p>
                </div>
            </div>

            <div class="person">
                <div class="person-description">
                    <h2>Naomi Harte</h2>
                    <div class="naomi-photo"></div>
                    <p>NAOMI HARTE is Professor in Speech Technology in the School of Engineering at Trinity College
                        Dublin,
                        Ireland. She is Co-PI and a founding member of the ADAPT SFI Centre. In ADAPT, she has led a
                        major
                        Research Theme centered on Multimodal Interaction involving researchers from Universities across
                        Ireland
                        and was instrumental in developing the future vision for the Centre for 2021-2026. She is also a
                        lead
                        academic of the hugely successful Sigmedia Research Group in the School of Engineering. She was
                        appointed as an SFI Engineering Initiative Lecturer in Digital Media in TCD in 2008 Her research
                        centres
                        around Human Speech Communication. She treats speech as something we both hear and see, with a
                        strong
                        multimodal aspect to her work. Her research involves the design and application of mathematical
                        algorithms to enhance or augment speech communication between humans and technology. Much of
                        that work
                        is underpinned by signal processing and machine learning, but also requires an understanding of
                        how
                        humans interact. Her current research projects include audio-visual speech recognition, speech
                        synthesis
                        evaluation, multimodal speech analysis, and birdsong. Her industrial background brings a
                        real-world
                        approach to her research. Prior to returning to academia, Naomi worked in high-tech start-ups in
                        the
                        field of DSP Systems Development, including her own company. She also previously worked in
                        McMaster
                        University in Canada. She was a Visiting Professor at ICSI in Berkeley in 2015, and became a
                        Fellow of
                        TCD in 2017. She earned a Google Faculty Award in 2018 and was shortlisted for the AI Ireland
                        Awards in
                        2019. She currently serves on the Editorial Board of Computer Speech and Language and Chair of
                        INTERSPEECH 2023.</p>
                </div>
            </div>


            <div class="person">
                <div class="person-description">
                    <h2>Shinji Watanabe</h2>
                    <div class="shi-photo"></div>
                    <p> SHINJI WATANABE (Fellow, IEEE) is an Associate Professor at Carnegie Mellon University,
                        Pittsburgh, PA.
                        He has been actively working on deep learning based speech enhancement and separation for speech
                        recognition applications, leading to the publication of more than 350 papers in peer-reviewed
                        journals
                        and conferences and receiving several awards. These include the best paper award from the IEEE
                        ASRU in
                        2019 for joint neural modeling of speech separation, beamforming, and speech recognition, which
                        is
                        related to this proposal. He also contributes to community-driven challenge activities,
                        including CHiME
                        speech recognition and separation challenges, which are famous speech processing activities, as
                        an
                        organizer and active participant. He organized two IEEE JSTSP Special issues on Self-Supervised
                        Learning
                        for Speech and Audio Processing and Far-Field Speech Processing in the Era of Deep Learning. He
                        serves
                        as a Senior Area Editor of the IEEE Transactions on Audio Speech and Language Processing. He
                        was/has
                        been a member of several technical committees, including the APSIPA Speech, Language, and Audio
                        Technical Committee (SLA), IEEE Signal Processing Society Speech and Language Technical
                        Committee
                        (SLTC), and Machine Learning for Signal Processing Technical Committee (MLSP).</p>
                </div>
            </div>

            <div class="person">
                <div class="person-description">
                    <h2><u></u>Minje Kim</h2>
                    <div class="kim-photo"></div>
                    <p> MINJE KIM (Senior Member, IEEE) is an Associate Professor at Indiana University, where he leads
                        his
                        research group, Signals and AI Group in Engineering (SAIGE), and is affiliated with Data
                        Science,
                        Cognitive Science, Statistics, Luddy Center for Artificial Intelligence, and the Center for
                        Machine
                        Learning. He is also an Amazon Visiting Academic, working at Amazon Lab126. He earned his Ph.D.
                        in the
                        Dept. of Computer Science at the University of Illinois at Urbana-Champaign. Before joining
                        UIUC, He
                        worked as a researcher at ETRI, a national lab in Korea, from 2006 to 2011. During his career as
                        a
                        researcher, he has focused on developing machine learning models for audio signal processing
                        applications. He is a recipient of various awards, including the NSF Career Award (2021), IU
                        Trustees
                        Teaching Award (2021), the IEEE SPS Best Paper Award (2020), Google and Starkey’s grants for
                        outstanding
                        student papers in ICASSP 2013 and 2014, respectively, and the Richard T. Cheng Endowed
                        Fellowship from
                        UIUC in 2011. He is an IEEE Senior Member and also a member of the IEEE Audio and Acoustic
                        Signal
                        Processing Technical Committee (2018-2023). He is serving as Senior Area Editor for the IEEE/ACM
                        Transactions on Audio, Speech, and Language Processing, Associate Editor for EURASIP Journal of
                        Audio,
                        Speech, and Music Processing, and Consulting Associate Editor for IEEE Open Journal of Signal
                        Processing. He is on more than 60 patents as an inventor.</p>
                </div>
            </div>

        </section>

        <section class="border" id="references">
            <h2>References</h2>
            <ol class="bold-list">
                <li>D. Michelsanti, Z.-H. Tan, S.-X. Zhang, Y. Xu, M. Yu, D. Yu, and J. Jesper,<u><a
                            href="https://arxiv.org/abs/2008.09586"> “An Overview of
                            Deep-Learning-Based Audio-Visual Speech Enhancement and Separation,"</a></u> IEEE/ACM
                    Transactions on Audio,
                    Speech, and Language Processing, vol. 29, pp. 1368-1396, 2021.</li>

                <li>S.-Y. Chuang, H.-M. Wang, and Y. Tsao, <u><a
                            href="https://dl.acm.org/doi/abs/10.1109/TASLP.2022.3153265?af=R"> “Improved Lite
                            Audio-Visual Speech Enhancement,”</a></u> IEEE/ACM
                    Transactions on Audio, Speech and Language Processing, vol. 30, pp. 1345-1359, 2022</li>

                <li>J.-C. Hou, S.-S. Wang, Y.-H. Lai, Y. Tsao, H.-W. Chang, and H.-M. Wang, <u><a
                            href="https://arxiv.org/abs/1709.00944"> “Audio-visual Speech
                            Enhancement using Multimodal Deep Convolutional Neural Networks,”</a></u> IEEE Transactions
                    on Emerging Topics
                    in Computational Intelligence, vol. 2(2), pp. 117-128, 2018.</li>

                <li>C. Yu, K.-H. Hung, S.-S. Wang, Y. Tsao, and J.-w. Hung,<u><a
                            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340386">“Time-Domain Multi-modal
                            Bone/air Conducted
                            Speech Enhancement,”</a></u>IEEE Signal Processing Letters, vol. 27, pp. 1035-1039, 2020.
                </li>

                <li>L. A. Passos, J. P. Papa., J. D. Ser. A. Hussain, and A. Adeel,<a
                        href="https://www.researchgate.net/publication/362601492_Multimodal_Audio-Visual_Information_Fusion_Using_Canonical-Correlated_Graph_Neural_Network_for_Energy-Efficient_Speech_Enhancement">
                        “Multimodal Audio-visual Information
                        Fusion using Canonical-correlated Graph Neural Networks for Energy-efficient Speech
                        Enhancement,”</a>
                    Information Fusion, vol. 90, pp. 1-11, 2023.</li>

                <li>A. Adeel, M. Gogate, A. Hussain, and W. M. Whitmer,<u><a
                            href="https://dblp.org/db/journals/tetci/tetci5.html"> “Lip-reading Driven Deep Learning
                            Approach for
                            Speech Enhancement,”</a></u>IEEE Transactions on Emerging Topics in Computational
                    Intelligence, vol. 5, no. 3,
                    pp. 481-490, 2021</li>

                <li>M. Gogate, K. Dashtipour, and A. Hussain, <u><a
                            href="https://www.stir.ac.uk/research/hub/publication/1608303"> “CochleaNet: A Robust
                            Language-independent Audio-Visual
                            Model for Speech Enhancement,”</a> </u>Information Fusion, vol. 63, pp. 273-285, 2020.</li>

                <li>J. Reverdy, S. O’Connor Russell, L. Duquenne, D. Garaialde, B. R. Cowan, and N. Harte.<u><a
                            href="https://aclanthology.org/2022.lrec-1.268/"> “RoomReader: A
                            Multimodal Corpus of Online Multiparty Conversational Interactions,”</a></u> in Proc. ELRA
                    2022.</li>

                <li>N. Harte and E. Gillen,<u><a href="https://ieeexplore.ieee.org/document/7050271?denied=">
                            “TCD-TIMIT: An Audio-Visual Corpus of Continuous Speech,”</a> </u> IEEE Transactions
                    on Multimedia, vol. 17, no. 5, pp. 603-615, 2015.</li>

                <li>T. Afouras, J. S. Chung, A., O. Vinyals, and A. Zisserman,<u><a
                            href="https://www.computer.org/csdl/journal/tp/2022/12/08585066/17D45VtKiwZ"> “Deep
                            Audio-visual Speech Recognition”</a> </u>
                    IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44(12), pp. 8717-8727, 2018.
                </li>

                <li>Z. Yu, Z. Yin, D. Zhou, D. Wang, F. Wong, and B. Wang,<u><a href=""> “Talking Head Generation with
                            Probabilistic
                            Audio-to-visual Diffusion Priors,”</a></u> in Proc. CVPR 2022.</li>

                <li>J. Richter, S. Frintrop, and T. Gerkmann.<u><a href="https://arxiv.org/abs/2306.01432">
                            “Audio-visual Speech Enhancement with Score-Based
                            Generative Models,”</a></u> arXiv:2306.01432, 2023.</li>

                <li>Q. Zhu, L. Zhou, Z. Zhang, S. Liu, B. Jiao, J. Zhang, et al.,<u><a
                            href="https://arxiv.org/pdf/2211.11275.pdf"> “Vatlm: Visual-audio-text Pre-training
                            With Unified Masked Prediction for Speech Representation Learning,”</a></u> IEEE
                    Transactions on Multimedia.</li>

                <li>R. Tan, et al.<u><a href=""> “Language-Guided Audio-visual Source Separation via Trimodal
                            Consistency,”</a></u> in Proc.
                    CVPR 2023.</li>

                <li>I-C. Chen, et al.<u><a href="https://arxiv.org/abs/2210.17456"> “Audio-visual Speech Enhancement and
                            Separation by Utilizing Multi-modal
                            Self-supervised Embeddings,”</a> </u> in Proc. ICASSPW, 2023.</li>

                <li>J.-C. Chou, C.-M. Chien, and K. Livescu,<u><a href="https://arxiv.org/abs/2309.08030"> “AV2Wav:
                            Diffusion-Based Re-synthesis from Continuous
                            Self-supervised Features for Audio-Visual Speech Enhancement,”</a></u> arXiv:2309.08030,
                    2023.</li>

                <li>Y.-J. Lu, C.-Y. Chang, C. Yu, C.-F. Liu, J.-W. Hung, S. Watanabe, and Y. Tsao,<u><a
                            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164201"> “Improving Speech
                            Enhancement Performance by Leveraging Contextual Broad Phonetic Class Information,”</a></u>
                    IEEE/ACM
                    Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 2738 – 2750, 2023.</li>

                <li>Y. Li, and X. Zhang, <u><a
                            href="https://www.sciencedirect.com/science/article/pii/S0925231223005556"> “Lip
                            Landmark-based Audio-visual Speech Enhancement With Multimodal Feature
                            Fusion Network,”</a></u> Neurocomputing, vol. 549, 2023.</li>

                <li>Xu, Haitao, et al.<u><a
                            href="https://www.researchgate.net/publication/358836935_Improved_Lite_Audio-Visual_Speech_Enhancement">
                            “A Multi-Scale Feature Aggregation Based Lightweight Network for Audio-Visual
                            Speech Enhancement,”</a> </u> in Proc. ICASSP 2023.</li>

                <li>M. Chu, Y. Ma, Z. Fan, M. Yang, Z. Tao, and D. Wu,<u><a
                            href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4395061">“Gmasegan: A Global
                            Multi-Head Attention Speech
                            Enhancement Generative Adversarial Network,”</a> </u> SSRN 4395061.</li>

                <li>L.-C. Chen, et al. <u><a
                            href="https://scholars.ncu.edu.tw/en/publications/epg2s-speech-generation-and-speech-enhancement-based-on-electropa">“EPG2S:
                            Speech Generation and Speech Enhancement Based on Electropalatography and
                            Audio Signals using Multimodal Learning,”</a> </u> IEEE Signal Processing Letters, vol. 29,
                    pp. 2582-2586, 2022.</li>

                <li>K.-C. Wang, et al. <u><a href="">“EMGSE: Acoustic/EMG Fusion for Multimodal Speech Enhancement,”
                        </a></u> in Proc. ICASSP
                    2022.</li>

                <li>J. W. Hwang, J. Park, R. H. Park, and H. M. Park,<u><a
                            href="https://www.x-mol.net/paper/article/1671058590615191552">“Audio-visual Speech
                            Recognition Based on Joint
                            Training with Audio-visual Speech Enhancement for Robust Speech Recognition,”</a> </u>
                    Applied Acoustics, vol.
                    211, 2023.</li>

                <li>Z. Zhu, H. Yang, M. Tang, Z. Yang, S. E. Eskimez, and H. Wang,<u><a
                            href="https://arxiv.org/pdf/2303.07005.pdf"></a> </u> in Proc. ICASSP 2023.</li>

                <li>T. Yoshinaga, K. Tanaka, and S. Morishima,<u><a href="">“Real-Time Audio-Visual End-To-End
                            Speech Enhancement,”</a></u> “Audio-Visual Speech Enhancement with Selective
                    Off-Screen Speech Extraction,” arXiv:2306.06495.</li>

                <li>C. Valentini-Botinhao, A. L. A. Blanco, O. Klejch, and P. Bell,<u><a
                            href="https://www.research.ed.ac.uk/en/organisations/centre-for-speech-technology-research/publications/">
                            “Efficient Intelligibility
                            Evaluation Using Keyword Spotting: A Study on Audio-Visual Speech Enhancement,”</a></u> in
                    Proc. ICASSP 2023.</li>

                <li>L. A. Passos, et. al., <u><a href="https://arxiv.org/pdf/2209.03275.pdf">“Multimodal Speech
                            Enhancement using Burst Propagation,”</a></u> arXiv:2209.03275.</li>

                <li>K. Kinoshita, M. Delcroix, A. Ogawa, and T. Nakatani, <u><a
                            href="https://www.isca-speech.org/archive/interspeech_2015/kinoshita15_interspeech.html">“Text-informed
                            Speech Enhancement with Deep
                            Neural Networks,”</a></u> in Proc. Interspeech 2015.</li>

                <li>J. Wu et al.,<u><a href="">“Time Domain Audio Visual Speech Separation,”</a></u> in Proc. ASRU 2019.
                </li>

                <li>R. Gu, S.-X. Zhang, Y. Xu, L. Chen, Y. Zou, and D. Yu,<u><a href="">“Multimodal Multi-channel Target
                            Speech
                            Separation,”</a></u> IEEE Journal of Selected Topics in Signal Processing, vol. 14, no. 3,
                    pp. 530-541, 2020.</li>

                <li>Y.-L. Chien, H.-H. Chen, M.-C. Yen, S.-W. Tsai, H.-M. Wang, Y. Tsao, and T.-S. Chi,<u><a
                            href="https://www.isca-speech.org/archive/interspeech_2023/chien23_interspeech.html">“Audio-Visual
                            Mandarin Electrolaryngeal Speech Voice Conversion,”</a></u> in Proc. Interspeech 2023.</li>

                <li>C.-F. Liao, Y. Tsao, X. Lu, and H. Kawai,<u><a
                            href="https://www.isca-speech.org/archive/interspeech_2019/liao19_interspeech.html">“Incorporating Symbolic Sequential Modeling for
                            Speech
                            Enhancement,”</a> </u> in Proc. Interspeech 2019.</li>

                <li>H. Julien, J. Thomas, Z. Véronique, and B. Éric, <u><a
                            href="https://hal.science/hal-04037315v2/document">“Configurable EBEN: Extreme Bandwidth Extension
                            Network to Enhance Body-conducted Speech Capture,”</a></u> IEEE/ACM Transactions on Audio,
                    Speech, and Language
                    Processing, in press.</li>

                <li>J. Chen, M. Wang, X. L. Zhang, Z. Huang, and S. Rahardja,<u><a
                            href="https://rc.signalprocessingsociety.org/conferences/icassp-2022/spsicassp22vid0058">“End-to-end Multi-modal Speech Recognition
                            with Air and Bone Conducted Speech</a></u> , in Proc. ICASSP 2022.</li>

                <li> M. Wang, J. Chen, X. Zhang, Z. Huang, and S. Rahardja,<u><a
                            href="https://www.sciencedirect.com/science/article/pii/S0003682X22004327">“Multi-modal Speech Enhancement with
                            Bone-conducted Speech in Time Domain,”</a></u> Applied Acoustics, vol 200, 2022.</li>

                <li>H. Wang, X. Zhang, and D. Wang,<u><a href="">“Attention-based Fusion
                            for Bone-conducted and Air-conducted Speech
                            Enhancement in the Complex Domain,”</a></u> in Proc. ICASSP 2022.</li>

                <li>H. Wang, X. Zhang, and D. Wang,<u><a href="https://dl.acm.org/doi/abs/10.1109/TASLP.2022.3209943">“Fusing Bone-Conduction
                            and Air-Conduction Sensors for
                            Complex-Domain Speech Enhancement,”</a></u> IEEE/ACM Transactions on Audio, Speech, and
                    Language Processing,
                    vol. 30, pp. 3134-3143, 2022.</li>

                <li>Y.-W. Chen, K.-H. Hung, S.-Y. Chuang, J. Sherman, X. Lu, and Y. Tsao, <u><a
                            href="https://eurasip.org/Proceedings/Eusipco/Eusipco2021/pdfs/0000496.pdf">“A study of Incorporating
                            Articulatory Movement Information in Speech Enhancement,”</a></u> in Proc. EUSIPCO 2021.
                </li>

                <li>A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein.
                    <u><a href="https://arxiv.org/abs/1804.03619">“Looking to Listen at the Cocktail Party: A
                            Speaker-independent Audio-visual Model for Speech
                            Separation,”</a></u> ACM Transactions on Graphics, vol. 37(4):111, 2018.
                </li>

                <li>A. Gabbay, A. Shamir, and S. Peleg,<u><a href="https://www.isca-speech.org/archive/interspeech_2018/gabbay18_interspeech.html">“Visual Speech
                            Enhancement,”</a></u> in Proc. Interspeech 2018.</li>

                <li>M. Liuzzolino and K. Koishida.<u><a href="https://www.semanticscholar.org/paper/AV(SE)2%3A-Audio-Visual-Squeeze-Excite-Speech-Iuzzolino-Koishida/a389cc390483a3938c5cf46d21fc0cfecbd839bf">“AV(se)2: Audio-visual
                            Squeeze-excite Speech Enhancement,”</a></u> in Proc.
                    ICASSP 2020.</li>

                <li>D. Michelsanti, Z.-H. Tan, S. Sigurdsson, and J. Jensen.<u><a
                            href="https://vbn.aau.dk/en/publications/deep-learning-based-audio-visual-speech-enhancement-in-presence-o">“Deep-learning-based Audio-visual Speech
                            Enhancement in the Presence of Lombard Effect,”</a></u> Speech Communication, vol. 115, pp.
                    38–50, 2019.</li>

                <li>M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Horaud,<u><a
                            href="https://inria.hal.science/hal-02364900/file/av_vae-R2.pdf">“Audio-visual Speech
                    Enhancement using Conditional Variational Auto-encoders,”</a></u> IEEE/ACM Transactions on Audio,
                    Speech, and
                    Language Processing, vol. 28, pp.1788–1800, 2020. </li>

            </ol>
        </section>


        <section class="border">
            <h2>Tentative Dates</h2>
            <div class="section">
                <div class="timeline">
                    <div class="row">
                        <div class="info">Manuscript submission due:</div>
                        <div class="date">April 31, 2023</div>
                    </div>
                    <div class="row">
                        <div class="info">First review completed:</div>
                        <div class="date">July 30, 2024</div>
                    </div>
                    <div class="row">
                        <div class="info">Revised manuscript due:</div>
                        <div class="date">October 15, 2024</div>
                    </div>
                    <div class="row">
                        <div class="info">Second review completed:</div>
                        <div class="date">December 30, 2024</div>
                    </div>
                    <div class="row">
                        <div class="info">Final manuscript due:</div>
                        <div class="date">January 31, 2025</div>
                    </div>
                </div>
            </div>
        </section>

</body>

</html>